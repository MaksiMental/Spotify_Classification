{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X_train and y_train\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Create a copy of the DataFrame\n",
    "train_data_copy = train_data.copy() \n",
    "\n",
    "features_copy = train_data_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "if 'popularity' in features_copy:\n",
    "    features_copy.remove('popularity')\n",
    "\n",
    "for x in features_copy:\n",
    "    q75, q25 = np.percentile(train_data_copy.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    "\n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    "\n",
    "    # Replace data points that lie outside of the lower and the upper bound with a NULL value.\n",
    "    train_data_copy.loc[train_data_copy[x] < min,x] = np.nan\n",
    "    train_data_copy.loc[train_data_copy[x] > max,x] = np.nan\n",
    "\n",
    "# Check the sum of null values or missing values\n",
    "print(train_data_copy.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with np.nan values (outliers)\n",
    "train_data_copy = train_data_copy.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_copy.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate X_train_copy and y_train again\n",
    "X_train_copy = train_data_copy.drop(columns=[y_train.name])\n",
    "y_train_copy = train_data_copy[y_train.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_copy.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_copy.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "#from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define the pipelines\n",
    "dtr_pipe = make_pipeline_imb(SMOTE(random_state=42), preprocessing, DecisionTreeClassifier())\n",
    "rf_pipe = make_pipeline_imb(SMOTE(random_state=42), preprocessing, RandomForestClassifier())\n",
    "brf_pipe = make_pipeline_imb(SMOTE(random_state=42), preprocessing, BalancedRandomForestClassifier(n_estimators=50, sampling_strategy=\"all\", replacement=True, bootstrap=False, random_state=42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = hbgb_clf_pipe.score(X_train, y_train)\n",
    "print(f'Training score: {train_score}')\n",
    "\n",
    "\n",
    "test_score = hbgb_clf_pipe.score(X_test, y_test)\n",
    "print(f'Test score: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_hbgb = cross_val_predict(hbgb_clf_pipe, X_train, y_train, cv=5)\n",
    "\n",
    "plt.rc('font', size=9)  # extra code â€“ make the text smaller\n",
    "\n",
    "# Display the confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred_hbgb)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_train, y_train_pred_hbgb)\n",
    "precision = precision_score(y_train, y_train_pred_hbgb)\n",
    "recall = recall_score(y_train, y_train_pred_hbgb)\n",
    "f1 = f1_score(y_train, y_train_pred_hbgb)\n",
    "\n",
    "# Print the classification metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_pipe = make_pipeline_imb(SMOTE(random_state=42), preprocessing, AdaBoostClassifier(DecisionTreeClassifier(max_depth=200), algorithm='SAMME', learning_rate=1, random_state=42))\n",
    "\n",
    "ada_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_train, y_train_pred_ada)\n",
    "precision = precision_score(y_train, y_train_pred_ada)\n",
    "recall = recall_score(y_train, y_train_pred_ada)\n",
    "f1 = f1_score(y_train, y_train_pred_ada)\n",
    "\n",
    "# Print the classification metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
